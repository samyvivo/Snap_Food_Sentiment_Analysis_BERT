# -*- coding: utf-8 -*-
"""Snap_Food_BERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aoa1mIGsVY83YkpBcMBYXBlBzXethSue
"""

import numpy as np
import pandas as pd
import re
import string
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score,roc_auc_score, classification_report
import warnings
warnings.filterwarnings('ignore')
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from transformers import DataCollatorWithPadding
from datasets import Dataset
import torch
import torch.nn.functional as F
from transformers.utils import logging
from transformers import TrainerCallback
from transformers.utils import logging
import time
import os

os.environ["WANDB_DISABLED"] = "true"

# 1. Load your SnapFood dataset
df = pd.read_csv("https://raw.githubusercontent.com/samyvivo/Snap_Food_Sentiment_Analysis_fa/refs/heads/main/snappfood_sentiment_train.csv")

df = df[["comment", "label"]].dropna()
df["label"] = np.where(df["label"] == "SAD", 0, 1)
df["label"] = df["label"].astype(int)

persian_punc = "؟،؛«»ـ"
all_punc = string.punctuation + persian_punc

def clean(text):
    text = str(text)
    text = str(text).lower()
    text = re.sub(r'[^\u0600-\u06FF\s]', '', text)  # remove non-Persian characters
    text = re.sub("\[.*?\]", "", text)
    text = re.sub("<.*?>+", "", text)
    text = re.sub(f"[{re.escape(all_punc)}]", "", text)
    text = re.sub("[%s]" % re.escape(string.punctuation), "", text)
    text = re.sub("\n", "", text)
    text = re.sub("\w*\d\w*", "", text)
    text = text.replace("—", "")
    text = " ".join(text)
    return text

df["comment"] = df["comment"].apply(clean)

X = df["comment"].tolist()
y = df["label"].tolist()

# 2. Split dataset
train_texts, test_texts, train_labels, test_labels = train_test_split(
    X, y, test_size=0.2, random_state=42)

# 3. Tokenize using BERT tokenizer
model_name = "HooshvareLab/bert-fa-base-uncased"
tokenizer = BertTokenizer.from_pretrained(model_name)
train_encodings = tokenizer(train_texts, truncation=True, padding=True)
test_encodings = tokenizer(test_texts, truncation=True, padding=True)

# 4. Convert to HuggingFace Dataset format
train_dataset = Dataset.from_dict({
    "input_ids": train_encodings["input_ids"],
    "attention_mask": train_encodings["attention_mask"],
    "label": train_labels
})
test_dataset = Dataset.from_dict({
    "input_ids": test_encodings["input_ids"],
    "attention_mask": test_encodings["attention_mask"],
    "label": test_labels
})

# 5. Load pre-trained BERT model
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)

# 6. Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="epoch",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=2e-5,
    logging_dir="./logs",
    logging_steps=500,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    fp16=True,
)

# 7. Data collator
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    # Get predicted class labels
    preds = torch.argmax(torch.tensor(logits), axis=1)

    # Apply softmax to get probabilities
    probs = F.softmax(torch.tensor(logits), dim=1)[:, 1]  # Probability of class 1

    return {
        "accuracy": accuracy_score(labels, preds),
        "f1": f1_score(labels, preds),
        "roc_auc": roc_auc_score(labels, probs)
    }

# 8. Trainer instance
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# 9. Train the model
trainer.train()

trainer.save_model("./saved_model")
tokenizer.save_pretrained("./saved_model")

trainer.save_model(r"C:/Users/samyh/Desktop/BERT/snapfood-bert-model")

# Save tokenizer
tokenizer.save_pretrained(r"C:/Users/samyh/Desktop/BERT/snapfood-bert-model")

import shutil

# Save
trainer.save_model("./saved_model")
tokenizer.save_pretrained("./saved_model")

# Zip the directory
shutil.make_archive("snapfood_roberta_model", 'zip', "./saved_model")